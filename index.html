
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <!-- Start : Google Analytics Code -->
  <!-- <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-177079629-1', 'auto');
    ga('send', 'pageview');
  </script> -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-177079629-1"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-177079629-1');
  </script>

  <!-- End : Google Analytics Code -->
  <meta http-equiv='cache-control' content='no-cache'>
  <meta http-equiv='expires' content='0'>
  <meta http-equiv='pragma' content='no-cache'>

  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* Design Credits: Jon Barron and Abhishek Kar and Saurabh Gupta*/
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 400
  }
  heading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 17px; /* 19 */
    font-weight: 600 /* 1000 */
  }
  strong {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 600 /* 800 */
  }
  strongred {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    color: 'red' ;
    font-size: 16px
  }
  sectionheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    font-weight: 600
  }
  pageheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 38px;
    font-weight: 400
  }
  .ImageBorder
  {
      border-width: 1px;
      border-color: Black;
  }
  </style>
  <!-- <link rel="shortcut icon" href="https://www.cs.cmu.edu/sites/default/files/favicon_0.ico" type="image/vnd.microsoft.icon"> -->
  <link rel="shortcut icon" type="image/png" href="icons/meta-logo.svg" sizes="32x32">
  <script type="text/javascript" src="js/hidebib.js"></script>
  <title>Rawal Khirodkar</title>
  <meta name="Rawal Khirodkar's Homepage" http-equiv="Content-Type" content="Rawal Khirodkar's Homepage">
  <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
  <!-- Scramble Script by Jeff Donahue -->
  <script src="js/scramble.js"></script>
</head>

<body>
<table width="840" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr><td>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <p align="center">
    <!-- <font size="7">Rawal Khirodkar</font><br> -->
    <pageheading>Rawal Khirodkar</pageheading><br>
    <!-- <b>email</b>:&nbsp
    <font id="email" style="display:inline;">
      <noscript><i>Please enable Javascript to view</i></noscript>
    </font> -->
    <!-- <script>
    emailScramble = new scrambledString(document.getElementById('email'),
        'emailScramble', 'dumekc@ohr.iud.skrc',
        [7, 15, 14, 17, 2, 10, 9, 6, 3, 5, 16, 4, 19, 18, 12, 11, 8, 1, 13]);
    </script> -->
  </p>
<!-- rkhirodk -->
  <tr>
    <!-- <td width="32%" valign="top"><a href="images/rawal.jpg"><img src="images/rawal.jpg" width="100%" style="border-radius:15px"></a> -->
    <td width="32%" valign="top"><a href="images/rawal.jpeg"><img src="images/rawal.jpeg" width="100%" style="border-radius:15px"></a>
    <p align=center>
<!--     | <a href="docs/resume.pdf">Resume</a> |-->
     | <a href="docs/cv.pdf">CV</a> |
    <a href="https://scholar.google.com/citations?user=hS6M138AAAAJ&hl=en&oi=ao">Google Scholar</a> |<br/>|
    <a href="https://github.com/rawalkhirodkar">Github</a> |
    <a href="https://twitter.com/me_rawal">Twitter</a> |
    </p>
    </td>
    <td width="68%" valign="top" align="justify">
    <p>I am a Research Scientist at <a href="https://about.meta.com/realitylabs/" target="_blank">Meta</a>.
      I received my Ph.D. in Robotics (2017-2023) from <a href="https://www.ri.cmu.edu/" target="_blank">Carnegie Mellon University</a> for my work on
      <a href="https://www.ri.cmu.edu/app/uploads/2023/09/rkhirodk_thesis.pdf" target="_blank">multi-human 3D reconstruction from in-the-wild videos</a> advised by
      Prof. <a href="http://www.cs.cmu.edu/~kkitani/" target="_blank">Kris Kitani</a>. I obtained my <a href="docs/Btech_Report_Rawal.pdf"> Bachelors</a> (2013-2017) in Computer Science from
      <a href="https://www.cse.iitb.ac.in/" target="_blank"> IIT Bombay</a>.
      My Ph.D. research was supported in part by the <a href="https://www.mhrd.gov.in/" target="_blank">Gov. of India Fellowship</a> and
      the <a href="https://www.amazon.science/news-and-features/amazon-announces-2023-carnegie-mellon-university-graduate-research-fellows" target="_blank">Amazon Fellowship</a>.
    </p>

    <p>My research is focused on reconstructing photorealistic humans in 3D from real world videos. If you are interested in interning at Meta with me, please send an email with your resume and research interests.
    </td>
  </tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td>
    <sectionheading>&nbsp;&nbsp;News</sectionheading>
    <ul>
    <li> Sept, 2024. <a href="https://neurips.cc/">One paper accepted to NeurIPS 2024</a>.</li>
    <li> Aug, 2024. <a href="https://github.com/facebookresearch/sapiens">Sapiens accepted to ECCV 2024 as an oral presentation</a>.</li>
    <li> July, 2024. <a href="https://asia.siggraph.org/2024/">One paper accepted to SIGGRAPH Asia 2024</a>.</li>
    <li> June, 2024. <a href="https://egovis.github.io/awards/2022_2023/">EgoHumans received the EgoVis Distinguished Paper Award at CVPR 2024</a>.</li>
    <!-- <li> Feb, 2024. <a href="https://cvpr.thecvf.com/">Two papers accepted to CVPR 2024</a>.</li> -->
    <li> Jan, 2024. <a href="https://sites.google.com/view/3d-humans-cvpr2024">Co-organizing the Humans Workshop at CVPR 2024</a>.</li>
    <li> Dec, 2023. <a href="https://ai.meta.com/blog/ego-exo4d-video-learning-perception/">Ego-Exo4D accepted to CVPR 2024 as an oral presentation</a>.</li>
    <li> Sept, 2023. <a href="https://youtu.be/h8-skpm5qAU">Successfully defended my PhD</a>.</li>
    <li> July, 2023. <a href="https://rawalkhirodkar.github.io/egohumans">EgoHumans accepted to ICCV 2023 as an oral presentation</a>.</li>
    <!-- <li> Feb, 2023. <a href="https://cvpr2023.thecvf.com/">OCSORT accepted to CVPR 2023</a>.</li> -->
    <!-- <li> Nov, 2022. <a href="https://youtu.be/VI4o0d_oWP4?list=PLw28TsbHRy4RrYFqETfGv7FHQ7L6EYO8-&t=13">Completed my Ph.D. proposal</a>.</li> -->
<!--     <li> Aug, 2022. <a href="https://about.facebook.com/realitylabs/">Completed internship at Meta Reality Labs</a>.</li> -->
<!--     <li> March, 2022. <a href="https://cvpr2022.thecvf.com/">OCHMR accepted to CVPR 2022</a>.</li> -->
<!--     <li> July, 2021. <a href="http://iccv2021.thecvf.com/home/">Two papers accepted to ICCV 2021</a>.</li> -->
    </ul>
  </td></tr>
</table>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Selected Publications</sectionheading></td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">

  <tr>
    <td width="40%" valign="top"><a href="sapiens/sapiens.gif"><img src="sapiens/sapiens.gif" alt="sym" width="90%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p><a href="https://rawalkhirodkar.github.io/sapiens/" id="sapiens_eccv_2024">
      <heading>Sapiens: Foundation for Human Vision Models</heading></a><br>
      <strong>Rawal Khirodkar</strong>, Timur Bagautdinov, Julieta Martinez, Su Zhaoen, Austin James, Peter Selednik, Stuart Anderson, Shunsuke Saito<br>
      <i>ECCV 2024 <strong style="color: orange;">(Best Paper Candidate)</strong>. <i style="color: rgb(45, 85, 246);">Top 15 papers</i>.
      </p>

      <div class="paper" id="sapiens_eccv_2024">
        <a href="https://rawalkhirodkar.github.io/sapiens/">project page</a> |
        <a href="https://arxiv.org/pdf/2408.12569" target="_blank">pdf</a> |
        <a href="javascript:toggleblock('sapiens_abs')">abstract</a> |
        <a href="https://github.com/facebookresearch/sapiens">code</a>
        <p align="justify"> <i id="sapiens_abs">We present Sapiens, a family of models for four fundamental human-centric vision tasks -- 2D pose estimation, body-part segmentation, depth estimation, and surface normal prediction. Our models natively support 1K high-resolution inference and are extremely easy to adapt for individual tasks by simply fine-tuning models pretrained on over 300 million in-the-wild human images. We observe that, given the same computational budget, self-supervised pretraining on a curated dataset of human images significantly boosts the performance for a diverse set of human-centric tasks. The resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. Our simple model design also brings scalability -- model performance across tasks improves as we scale the number of parameters from 0.3 to 2 billion. Sapiens consistently surpasses existing baselines across various human-centric benchmarks. We achieve significant improvements over the prior state-of-the-art on Humans-5K (pose) by 7.6 mAP, Humans-2K (part-seg) by 17.1 mIoU, Hi4D (depth) by 22.4% relative RMSE, and THuman2 (normal) by 53.5% relative angular error</i></p>
        <pre xml:space="preserve">
          @misc{khirodkar2024_sapiens,
            title={Sapiens: Foundation for Human Vision Models},
            author={Khirodkar, Rawal and Bagautdinov, Timur and Martinez, Julieta and Zhaoen, Su and James, Austin and Selednik, Peter and Anderson, Stuart and Saito, Shunsuke},
            year={2024},
            eprint={2408.12569},
            archivePrefix={arXiv},
            primaryClass={cs.CV},
            url={https://arxiv.org/abs/2408.12569}
        }
        </pre>
      </div>
    </td>
  </tr>

  <tr>
    <td width="40%" valign="top"><a href="ego_exo4d/ego_exo4d.gif"><img src="ego_exo4d/ego_exo4d.gif" alt="sym" width="90%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p><a href="https://ego-exo4d-data.org/" id="ego_exo_cvpr_2024">
      <heading>Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives</heading></a><br>
      Kristen Grauman et al.<br>
      <i>CVPR 2024 <strong style="color: orange;">(Oral)</strong>. <i style="color: rgb(45, 85, 246);">Oral/Accepted: 3.3%</i>.
      </p>

      <div class="paper" id="ego_exo_cvpr_2024">
        <a href="https://ego-exo4d-data.org/">project page</a> |
        <a href="https://arxiv.org/pdf/2311.18259.pdf" target="_blank">pdf</a> |
        <a href="javascript:toggleblock('ego_exo_4d_abs')">abstract</a> |
        <a href="https://ai.meta.com/blog/ego-exo4d-video-learning-perception/">blog</a> |
        <a href="https://www.youtube.com/watch?v=GdooXEBAnI8">video</a>
        <p align="justify"> <i id="ego_exo_4d_abs">We present Ego-Exo4D, a diverse, large-scale multimodal multiview video dataset and benchmark challenge. Ego-Exo4D centers around simultaneously-captured egocentric and exocentric video of skilled human activities (e.g., sports, music, dance, bike repair). More than 800 participants from 13 cities worldwide performed these activities in 131 different natural scene contexts, yielding long-form captures from 1 to 42 minutes each and 1,422 hours of video combined. The multimodal nature of the dataset is unprecedented: the video is accompanied by multichannel audio, eye gaze, 3D point clouds, camera poses, IMU, and multiple paired language descriptions -- including a novel "expert commentary" done by coaches and teachers and tailored to the skilled-activity domain. To push the frontier of first-person video understanding of skilled human activity, we also present a suite of benchmark tasks and their annotations, including fine-grained activity understanding, proficiency estimation, cross-view translation, and 3D hand/body pose. All resources will be open sourced to fuel new research in the community.</i></p>
        <pre xml:space="preserve">
          @article{grauman2023ego,
            title={Ego-exo4d: Understanding skilled human activity from first-and third-person perspectives},
            author={Grauman, Kristen and Westbury, Andrew and Torresani, Lorenzo and Kitani, Kris and Malik, Jitendra and Afouras, Triantafyllos and Ashutosh, Kumar and Baiyya, Vijay and Bansal, Siddhant and Boote, Bikram and others},
            journal={arXiv preprint arXiv:2311.18259},
            year={2023}
          }
        </pre>
      </div>
    </td>
  </tr>

  <tr>
    <td width="40%" valign="top"><a href="simxr/simxr.gif"><img src="simxr/simxr.gif" alt="sym" width="90%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p><a href="https://arxiv.org/abs/2210.05387" id="cvpr_2024">
      <heading>Real-Time Simulated Avatar from Head-Mounted Sensors</heading></a><br>
      Zhengyi Luo, Jinkun Cao, <strong>Rawal Khirodkar</strong>, Alexander Winkler, Jing Huang, Kris Kitani, Weipeng Xu<br>
      <i>CVPR 2024 <strong style="color: orange;">(Highlight)</strong>. <i style="color: rgb(45, 85, 246);">Highlight/Accepted: 11.9%</i>.
      </p>

      <div class="paper" id="simxr">
        <a href="https://www.zhengyiluo.com/SimXR/">project page</a>
      </div>
    </td>
  </tr>

  <!-- <tr>
    <td width="40%" valign="top"><a href="lidar/lidar.png"><img src="lidar/lidar.png" alt="sym" width="90%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p><a href="https://arxiv.org/pdf/2401.15616.pdf" id="lidar_arxiv_2024">
      <heading>Multi-Person 3D Pose Estimation from Multi-View Uncalibrated Depth Cameras</heading></a><br>
      Yu-Jhe Li, Yan Xu, <strong>Rawal Khirodkar</strong>, Jinhyung Park, Kris Kitani<br>
      arXiv, 2024
      </p>

      <div class="paper" id="lidar_arxiv_2024">
        <a href="https://arxiv.org/pdf/2210.05387.pdf" target="_blank">pdf</a> |
        <a href="javascript:toggleblock('lidar_arxiv_2024_abs')">abstract</a> |
        <a shape="rect" href="javascript:togglebib('lidar_arxiv_2024')" class="togglebib">bibtex</a>

        <p align="justify"> <i id="lidar_arxiv_2024_abs">We tackle the task of multi-view, multi-person 3D human
          pose estimation from a limited number of uncalibrated depth
          cameras. Recently, many approaches have been proposed
          for 3D human pose estimation from multi-view RGB cameras. However, these works (1) assume the number of RGB
          camera views is large enough for 3D reconstruction, (2)
          the cameras are calibrated, and (3) rely on ground truth
          3D poses for training their regression model. In this work,
          we propose to leverage sparse, uncalibrated depth cameras
          providing RGBD video streams for 3D human pose estimation. We present a simple pipeline for Multi-View Depth
          Human Pose Estimation (MVD-HPE) for jointly predicting
          the camera poses and 3D human poses without training a
          deep 3D human pose regression model. This framework
          utilizes 3D Re-ID appearance features from RGBD images
          to formulate more accurate correspondences (for deriving
          camera positions) compared to using RGB-only features. We
          further propose (1) depth-guided camera-pose estimation
          by leveraging 3D rigid transformations as guidance and (2)
          depth-constrained 3D human pose estimation by utilizing
          depth-projected 3D points as an alternative objective for
          optimization. In order to evaluate our proposed pipeline,
          we collect three video sets of RGBD videos recorded from
          multiple sparse-view depth cameras, and ground truth 3D
          poses are manually annotated. Experiments show that our
          proposed method outperforms the current 3D human pose
          regression-free pipelines in terms of both camera pose estimation and 3D human pose estimation..</i></p>

        <pre xml:space="preserve">
          @article{li2024multi,
            title={Multi-Person 3D Pose Estimation from Multi-View Uncalibrated Depth Cameras},
            author={Li, Yu-Jhe and Xu, Yan and Khirodkar, Rawal and Park, Jinhyung and Kitani, Kris},
            journal={arXiv preprint arXiv:2401.15616},
            year={2024}
          }
        </pre>
      </div>

    </td>
  </tr> -->

  <tr>
    <td width="40%" valign="top"><a href="phd_thesis/grappling2.gif"><img src="phd_thesis/grappling2.gif" alt="sym" width="90%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p><a href="https://arxiv.org/abs/2210.05387" id="cmu_phd_thesis">
      <heading>Multi-Human 3D Reconstruction from In-the-Wild Videos</heading></a><br>
      Committee members: Kris Kitani, Deva Ramanan, Angjoo Kanazawa, Shubham Tulsiani<br>
      </p>

      <div class="paper" id="cmu_phd_thesis">
        <a href="https://www.ri.cmu.edu/publications/multi-human-3d-reconstruction-from-in-the-wild-videos/">Ph.D. Thesis, Carnegie Mellon University, 2023</a>
      </div>
    </td>
  </tr>

  <tr>
      <td width="40%" valign="top"><a href="egohumans/files/egohumans.gif"><img src="egohumans/files/egohumans.gif" alt="sym" width="90%" style="border-radius:15px"></a></td>
      <td width="67%" valign="top">
        <p><a href="https://rawalkhirodkar.github.io/egohumans" id="iccv_2023">
        <heading>EgoHumans: An Egocentric 3D Multi-Human Benchmark</heading></a><br>
        <strong>Rawal Khirodkar</strong>, Aayush Bansal, Lingni Ma, Richard Newcombe, Minh Vo, Kris Kitani<br>
        ICCV 2023 <strong style="color: orange;">(Oral)</strong>. <i style="color: rgb(45, 85, 246);">Oral/Accepted: 9.0%</i>.
        </p>

        <div class="paper" id="egohumans">
          <a href="https://rawalkhirodkar.github.io/egohumans">project page</a> |
          <a href="egohumans/files/paper.pdf" target="_blank">pdf</a> |
          <a href="javascript:toggleblock('egohumans_abs')">abstract</a> |
          <a shape="rect" href="javascript:togglebib('egohumans')" class="togglebib">bibtex</a> |
          <a href="https://github.com/rawalkhirodkar/egohumans">code</a>
          <p align="justify"> <i id="egohumans_abs">We present EgoHumans, a new multi-view multi-human video benchmark to advance the state-of-the-art of egocentric human 3D pose estimation and tracking. Existing egocentric benchmarks either capture single subject or indoor-only scenarios, which limit the generalization of computer vision algorithms for real-world applications. We propose a novel 3D capture setup to construct a comprehensive egocentric multi-human benchmark in the wild with annotations to support diverse tasks such as human detection, tracking, 2D/3D pose estimation, and mesh recovery. We leverage consumer-grade wearable camera-equipped glasses for the egocentric view, which enables us to capture dynamic activities like playing soccer, fencing, volleyball, etc. Furthermore, our multi-view setup generates accurate 3D ground truth even under severe or complete occlusion. The dataset consists of more than 125k egocentric images, spanning diverse scenes with a particular focus on challenging and unchoreographed multi-human activities and fast-moving egocentric views. We rigorously evaluate existing state-of-the-art methods and highlight their limitations in the egocentric scenario, specifically on multi-human tracking. To address such limitations, we propose EgoFormer, a novel approach with a multi-stream transformer architecture and explicit 3D spatial reasoning to estimate and track the human pose. EgoFormer significantly outperforms prior art by 13.6 IDF1 and 9.3 HOTA on the EgoHumans dataset.</i></p>
          <pre xml:space="preserve">
            @article{khirodkar2023egohumans,
              title={EgoHumans: An Egocentric 3D Multi-Human Benchmark},
              author={Khirodkar, Rawal and Bansal, Aayush and Ma, Lingni and Newcombe, Richard and Vo, Minh and Kitani, Kris},
              journal={arXiv preprint arXiv:2305.16487},
              year={2023}
            }
          </pre>
        </div>
      </td>
    </tr>


    <tr>
    <td width="40%" valign="top"><a href="images/ocsort.png"><img src="images/ocsort.png" alt="sym" width="90%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p><a href="https://arxiv.org/abs/2203.14360" id="cvpr_2023">
      <heading>Observation-Centric SORT: Rethinking SORT for Robust Multi-Object Tracking</heading></a><br>
      Jinkun Cao, Xinshuo Weng, <strong>Rawal Khirodkar</strong>, Jiangmiao Pang, Kris Kitani<br>
      CVPR 2023
      </p>

      <div class="paper" id="ocsort">
        <a href="https://arxiv.org/abs/2203.14360" target="_blank">pdf</a> |
        <a href="javascript:toggleblock('ocsort_abs')">abstract</a> |
        <a shape="rect" href="javascript:togglebib('ocsort')" class="togglebib">bibtex</a> |
        <a href="https://github.com/noahcao/OC_SORT">code</a>

        <p align="justify"> <i id="ocsort_abs">Multi-Object Tracking (MOT) has rapidly progressed with the development of object detection and re-identification. However, motion modeling, which facilitates object association by forecasting short-term trajectories with past observations, has been relatively under-explored in recent years. Current motion models in MOT typically assume that the object motion is linear in a small time window and needs continuous observations, so these methods are sensitive to occlusions and non-linear motion and require high frame-rate videos. In this work, we show that a simple motion model can obtain state-of-the-art tracking performance without other cues like appearance. We emphasize the role of "observation" when recovering tracks from being lost and reducing the error accumulated by linear motion models during the lost period. We thus name the proposed method as Observation-Centric SORT, OC-SORT for short. It remains simple, online, and real-time but improves robustness over occlusion and non-linear motion. It achieves 63.2 and 62.1 HOTA on MOT17 and MOT20, respectively, surpassing all published methods. It also sets new states of the art on KITTI Pedestrian Tracking and DanceTrack where the object motion is highly non-linear.</i></p>

        <pre xml:space="preserve">
         @article{cao2022observation,
            title={Observation-centric sort: Rethinking sort for robust multi-object tracking},
            author={Cao, Jinkun and Weng, Xinshuo and Khirodkar, Rawal and Pang, Jiangmiao and Kitani, Kris},
            journal={arXiv preprint arXiv:2203.14360},
            year={2022}
          }
        </pre>
      </div>
    </td>
  </tr>

<!-- <tr>
    <td width="40%" valign="top"><a href="images/seq_ens.png"><img src="images/seq_ens.png" alt="sym" width="90%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p><a href="https://arxiv.org/abs/2210.05387" id="arxiv_2022">
      <heading>Sequential Ensembling for Semantic Segmentation</heading></a><br>
      <strong>Rawal Khirodkar</strong>, Brandon Smith, Siddhartha Chandra, Amit Agrawal, Antonio Criminisi<br>
      arXiv, 2022
      </p>

      <div class="paper" id="seq_ens">
        <a href="https://arxiv.org/pdf/2210.05387.pdf" target="_blank">pdf</a> |
        <a href="javascript:toggleblock('seq_ens_abs')">abstract</a> |
        <a shape="rect" href="javascript:togglebib('seq_ens')" class="togglebib">bibtex</a>

        <p align="justify"> <i id="seq_ens_abs">Ensemble approaches for deep-learning-based semantic segmentation remain insufficiently explored despite the proliferation of competitive benchmarks and downstream applications. In this work, we explore and benchmark the popular ensembling approach of combining predictions of multiple, independently-trained, state-of-the-art models at test time on popular datasets. Furthermore, we propose a novel method inspired by boosting to sequentially ensemble networks that significantly outperforms the naive ensemble baseline. Our approach trains a cascade of models conditioned on class probabilities predicted by the previous model as an additional input. A key benefit of this approach is that it allows for dynamic computation offloading, which helps deploy models on mobile devices. Our proposed novel ADaptive modulatiON (ADON) block allows spatial feature modulation at various layers using previous-stage probabilities. Our approach does not require sophisticated sample selection strategies during training and works with multiple neural architectures. We significantly improve over the naive ensemble baseline on challenging datasets such as Cityscapes, ADE-20K, COCO-Stuff, and PASCAL-Context and set a new state-of-the-art.</i></p>

        <pre xml:space="preserve">
         @article{khirodkar2022sequential,
              title={Sequential Ensembling for Semantic Segmentation},
              author={Khirodkar, Rawal and Smith, Brandon and Chandra, Siddhartha and Agrawal, Amit and Criminisi, Antonio},
              journal={arXiv preprint arXiv:2210.05387},
              year={2022}
            }
        </pre>
      </div>
    </td>
  </tr> -->

  <tr>
    <td width="40%" valign="top"><a href="images/cvpr22_ochmr.png"><img src="images/cvpr22_ochmr.png" alt="sym" width="90%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p><a href="https://arxiv.org/abs/2203.13349" id="CVPR22">
      <heading>Occluded Human Mesh Recovery</heading></a><br>
      <strong>Rawal Khirodkar</strong>, Shashank Tripathi, Kris Kitani<br>
      CVPR 2022
      </p>

      <div class="paper" id="cvpr22_ochmr">
        <a href="https://rawalkhirodkar.github.io/ochmr/">project page</a> |
        <a href="ochmr/files/paper.pdf" target="_blank">pdf</a> |
        <a href="javascript:toggleblock('cvpr22_ochmr_abs')">abstract</a> |
        <a shape="rect" href="javascript:togglebib('cvpr22_ochmr')" class="togglebib">bibtex</a>

        <p align="justify"> <i id="cvpr22_ochmr_abs">Top-down methods for monocular human mesh recovery have two stages: (1) detect human bounding boxes; (2) treat each bounding box as an independent single-human mesh recovery task. Unfortunately, the single-human assumption does not hold in images with multi-human occlusion and crowding. Consequently, top-down methods have difficulties in recovering accurate 3D human meshes under severe person-person occlusion. To address this, we present Occluded Human Mesh Recovery (OCHMR) - a novel top-down mesh recovery approach that incorporates image spatial context to overcome the limitations of the single-human assumption. The approach is conceptually simple and can be applied to any existing top-down architecture. Along with the input image, we condition the top-down model on spatial context from the image in the form of body-center heatmaps. To reason from the predicted body centermaps, we introduce Contextual Normalization (CoNorm) blocks to adaptively modulate intermediate features of the top-down model. The contextual conditioning helps our model disambiguate between two severely overlapping human bounding-boxes, making it robust to multi-person occlusion. Compared with state-of-the-art methods, OCHMR achieves superior performance on challenging multi-person benchmarks like 3DPW, CrowdPose and OCHuman. Specifically, our proposed contextual reasoning architecture applied to the SPIN model with ResNet-50 backbone results in 75.2 PMPJPE on 3DPW-PC, 23.6 AP on CrowdPose and 37.7 AP on OCHuman datasets, a significant improvement of 6.9 mm, 6.4 AP and 20.8 AP respectively over the baseline.</i></p>

        <pre xml:space="preserve">
         @article{khirodkar2022occluded,
          title={Occluded Human Mesh Recovery},
          author={Khirodkar, Rawal and Tripathi, Shashank and Kitani, Kris},
          journal={arXiv preprint arXiv:2203.13349},
          year={2022}
        }
        </pre>
      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top"><a href="images/iccv21_mip.png"><img src="images/iccv21_mip.png" alt="sym" width="90%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p><a href="https://arxiv.org/abs/2101.11223" id="ICCV21">
      <heading>Multi-Instance Pose Networks: Rethinking Top-Down Pose Estimation</heading></a><br>
      <strong>Rawal Khirodkar</strong>, Visesh Chari, Amit Agrawal, Ambrish Tyagi<br>
      ICCV 2021
      </p>

      <div class="paper" id="iccv21_mip">
        <a href="https://rawalkhirodkar.github.io/mipnet/">project page</a> |
        <a href="https://www.amazon.science/publications/multi-instance-pose-networks-rethinking-top-down-pose-estimation">Amazon Science</a> |
        <a href="mipnet/files/paper.pdf" target="_blank">pdf</a> |
        <a href="javascript:toggleblock('iccv21_mip_abs')">abstract</a> |
        <a shape="rect" href="javascript:togglebib('iccv21_mip')" class="togglebib">bibtex</a> |
        <a href="https://github.com/rawalkhirodkar/MIPNet">code</a>

        <p align="justify"> <i id="iccv21_mip_abs">A key assumption of top-down human pose estimation approaches is their expectation of having a single person/instance present in the input bounding box. This often leads to failures in crowded scenes with occlusions. We propose a novel solution to overcome the limitations of this fundamental assumption. Our Multi-Instance Pose Network (MIPNet) allows for predicting multiple 2D pose instances within a given bounding box. We introduce a Multi-Instance Modulation Block (MIMB) that can adaptively modulate channel-wise feature responses for each instance and is parameter efficient. We demonstrate the efficacy of our approach by evaluating on COCO, CrowdPose, and OCHuman datasets. Specifically, we achieve 70.0 AP on CrowdPose and 42.5 AP on OCHuman test sets, a significant improvement of 2.4 AP and 6.5 AP over the prior art, respectively. When using ground truth bounding boxes for inference, MIPNet achieves an improvement of 0.7 AP on COCO, 0.9 AP on CrowdPose, and 9.1 AP on OCHuman validation sets compared to HRNet. Interestingly, when fewer, high confidence bounding boxes are used, HRNet's performance degrades (by 5 AP) on OCHuman, whereas MIPNet maintains a relatively stable performance (drop of 1 AP) for the same inputs.</i></p>

        <pre xml:space="preserve">
         @InProceedings{Khirodkar_2021_ICCV,
              author    = {Khirodkar, Rawal and Chari, Visesh and Agrawal, Amit and Tyagi, Ambrish},
              title     = {Multi-Instance Pose Networks: Rethinking Top-Down Pose Estimation},
              booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
              month     = {October},
              year      = {2021},
              pages     = {3122-3131}
          }
        </pre>
      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top"><a href="images/iccv21_repose.png"><img src="images/iccv21_repose.png" alt="sym" width="90%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p><a href="https://arxiv.org/pdf/2104.00633" id="ICCV21">
      <heading>RePOSE: Fast 6D Object Pose Refinement via Deep Texture Rendering</heading></a><br>
      Shun Iwase, Xingyu Liu, <strong>Rawal Khirodkar</strong>, Rio Yokota, Kris M. Kitani<br>
      ICCV 2021
      </p>

      <div class="paper" id="iccv21_repose">
        <a href="https://arxiv.org/pdf/2104.00633.pdf" target="_blank">pdf</a> |
        <a href="javascript:toggleblock('iccv21_repose_abs')">abstract</a> |
        <a shape="rect" href="javascript:togglebib('iccv21_repose')" class="togglebib">bibtex</a> |
        <a href="https://github.com/sh8/RePOSE">code</a>

        <p align="justify"> <i id="iccv21_repose_abs">The use of iterative pose refinement is a critical processing step for 6D object pose estimation, and its performance depends greatly on one's choice of image representation. Image representations learned via deep convolutional neural networks (CNN) are currently the method of choice as they are able to robustly encode object keypoint locations. However, CNN-based image representations are computational expensive to use for iterative pose refinement, as they require that image features are extracted using a deep network, once for the input image and multiple times for rendered images during the refinement process. Instead of using a CNN to extract image features from a rendered RGB image, we propose to directly render a deep feature image. We call this deep texture rendering, where a shallow multi-layer perceptron is used to directly regress a view invariant image representation of an object. Using an estimate of the pose and deep texture rendering, our system can render an image representation in under 1ms. This image representation is optimized such that it makes it easier to perform nonlinear 6D pose estimation by adding a differentiable Levenberg-Marquardt optimization network and back-propagating the 6D pose alignment error. We call our method, RePOSE, a Real-time Iterative Rendering and Refinement algorithm for 6D POSE estimation. RePOSE runs at 71 FPS and achieves state-of-the-art accuracy of 51.6% on the Occlusion LineMOD dataset - a 4.1% absolute improvement over the prior art, and comparable performance on the YCB-Video dataset with a much faster runtime than the other pose refinement methods.</i></p>

        <pre xml:space="preserve">
          @InProceedings{Iwase_2021_ICCV,
              author    = {Iwase, Shun and Liu, Xingyu and Khirodkar, Rawal and Yokota, Rio and Kitani, Kris M.},
              title     = {RePOSE: Fast 6D Object Pose Refinement via Deep Texture Rendering},
              booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
              month     = {October},
              year      = {2021},
              pages     = {3303-3312}
          }
        </pre>
      </div>
    </td>
  </tr>


  <!-- <tr>
    <td width="33%" valign="top"><a href="images/publications/adr/1.png"><img src="images/publications/adr/1.png" alt="sym" width="90%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p><a href="https://arxiv.org/pdf/1812.00491" id="ADR">
      <heading>Adversarial Domain Randomization</heading></a><br>
      <strong>Rawal Khirodkar</strong>, Kris M. Kitani<br>
      arXiv, 2018
      </p>

      <div class="paper" id="adr">
        <a href="https://arxiv.org/pdf/1812.00491.pdf" target="_blank">pdf</a> |
        <a href="javascript:toggleblock('adr_abs')">abstract</a> |
        <a shape="rect" href="javascript:togglebib('adr')" class="togglebib">bibtex</a>

        <p align="justify"> <i id="adr_abs">Domain Randomization (DR) is known to require a significant amount of training data for good performance. We argue that this is due to DR's strategy of random data generation using a uniform distribution over simulation parameters, as a result, DR often generates samples which are uninformative for the learner. In this work, we theoretically analyze DR using ideas from multi-source domain adaptation. Based on our findings, we propose Adversarial Domain Randomization (ADR) as an efficient variant of DR which generates adversarial samples with respect to the learner during training. We implement ADR as a policy whose action space is the quantized simulation parameter space. At each iteration, the policy's action generates labeled data and the reward is set as negative of learner's loss on this data. As a result, we observe ADR frequently generates novel samples for the learner like truncated and occluded objects for object detection and confusing classes for image classification. We perform evaluations on datasets like CLEVR, Syn2Real, and VIRAT for various tasks where we demonstrate that ADR outperforms DR by generating fewer data samples.</i></p>

        <pre xml:space="preserve">
          @article{DBLP:journals/corr/abs-1812-00491,
            author    = {Rawal Khirodkar and
                         Donghyun Yoo and
                         Kris M. Kitani},
            title     = {{VADRA:} Visual Adversarial Domain Randomization and Augmentation},
            journal   = {CoRR},
            volume    = {abs/1812.00491},
            year      = {2018},
            url       = {http://arxiv.org/abs/1812.00491},
            archivePrefix = {arXiv},
            eprint    = {1812.00491},
            timestamp = {Tue, 01 Jan 2019 15:01:25 +0100},
            biburl    = {https://dblp.org/rec/journals/corr/abs-1812-00491.bib},
            bibsource = {dblp computer science bibliography, https://dblp.org}
          }
        </pre>
      </div>
    </td>
  </tr> -->

  <!-- <tr>
    <td width="33%" valign="top"><a href="images/wacv19.png"><img src="images/wacv19.png" alt="sym" width="90%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p><a href="https://arxiv.org/abs/1811.05939" id="WACV19">
      <heading>Domain Randomization for Scene-Specific Car Detection and Pose Estimation</heading></a><br>
      <strong>Rawal Khirodkar</strong>, Donghyun Yoo, Kris M. Kitani<br>
      WACV 2019
      </p>

      <div class="paper" id="wacv19_dr">
        <a href="https://arxiv.org/pdf/1811.05939.pdf" target="_blank">pdf</a> |
        <a href="javascript:toggleblock('wacv19_dr_abs')">abstract</a> |
        <a shape="rect" href="javascript:togglebib('wacv19_dr')" class="togglebib">bibtex</a> |
        <a href="https://github.com/rawalkhirodkar/car_pose">code</a>

        <p align="justify"> <i id="wacv19_dr_abs">We address the issue of domain gap when making use of synthetic data to train a scene-specific object detector and pose estimator. While previous works have shown that the constraints of learning a scene-specific model can be leveraged to create geometrically and photometrically consistent synthetic data, care must be taken to design synthetic content which is as close as possible to the real-world data distribution. In this work, we propose to solve domain gap through the use of appearance randomization to generate a wide range of synthetic objects to span the space of realistic images for training. An ablation study of our results is presented to delineate the individual contribution of different components in the randomization process. We evaluate our method on VIRAT, UA-DETRAC, EPFL-Car datasets, where we demonstrate that using scene specific domain randomized synthetic data is better than fine-tuning off-the-shelf models on limited real data.</i></p>

        <pre xml:space="preserve">
          @inproceedings{khirodkar2019domain,
            title={Domain randomization for scene-specific car detection and pose estimation},
            author={Khirodkar, Rawal and Yoo, Donghyun and Kitani, Kris},
            booktitle={2019 IEEE Winter Conference on Applications of Computer Vision (WACV)},
            pages={1932--1940},
            year={2019},
            organization={IEEE}
          }
        </pre>
      </div>
    </td>
  </tr> -->

</table>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>Professional Services</sectionheading></td></tr>
  <tr>
    <td width="33%" valign="top"><a href="images/cvf.jpg"><img src="images/cvf.jpg" alt="sym" width="90%" style="border-radius:15px"></a></td>
    <td>
      <ul>
        <li>CVPR (2018, 2020, 2021, 2022, 2023, 2024)</li>
        <li>ICCV (2019, 2021, 2023)</li>
        <li>ECCV (2020, 2022)</li>
        <li>NeurIPS (2022, 2023)</li>
        <li>ICLR (2022, 2023, 2024)</li>
        <li>IJCV (2022)</li>
        <li>TPAMI (2023)</li>
        <li>WACV (2021, 2022, 2023, 2024)</li>
        <li>ACCV (2018, 2020, 2021)</li>
      </ul>
    </td>
  </tr>

</table>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
    <tr><td><br><p align="right"><font size="1.5">
    Template modified from <a href="http://www.cs.berkeley.edu/~barron/">this</a>, and <a href="http://jeffdonahue.com/">this</a>
    </font></p></td></tr>
</table>

  </td></tr>
</table>



<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>

<script xml:space="preserve" language="JavaScript">
hideblock('sapiens_abs');
// hideblock('lidar_arxiv_2024_abs');
hideblock('ego_exo_4d_abs');
hideblock('egohumans_abs');
hideblock('ocsort_abs');
// hideblock('seq_ens_abs');
hideblock('cvpr22_ochmr_abs');
hideblock('iccv21_mip_abs');
hideblock('iccv21_repose_abs');
// hideblock('adr_abs');
// hideblock('wacv19_dr_abs');
</script>

</body>

</html>
