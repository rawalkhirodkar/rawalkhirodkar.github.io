
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <!-- Start : Google Analytics Code -->
  <!-- <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-177079629-1', 'auto');
    ga('send', 'pageview');
  </script> -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-177079629-1"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-177079629-1');
  </script>

  <!-- End : Google Analytics Code -->
  <meta http-equiv='cache-control' content='no-cache'> 
  <meta http-equiv='expires' content='0'> 
  <meta http-equiv='pragma' content='no-cache'>
  
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* Design Credits: Jon Barron and Abhishek Kar and Saurabh Gupta*/
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 400
  }
  heading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 17px; /* 19 */
    font-weight: 600 /* 1000 */
  }
  strong {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 600 /* 800 */
  }
  strongred {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    color: 'red' ;
    font-size: 16px
  }
  sectionheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    font-weight: 600
  }
  pageheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 38px;
    font-weight: 400
  }
  .ImageBorder
  {
      border-width: 1px;
      border-color: Black;
  }
  </style>
  <link rel="shortcut icon" href="https://www.cs.cmu.edu/sites/default/files/favicon_0.ico" type="image/vnd.microsoft.icon">
  <script type="text/javascript" src="js/hidebib.js"></script>
  <title>Rawal Khirodkar</title>
  <meta name="Rawal Khirodkar's Homepage" http-equiv="Content-Type" content="Rawal Khirodkar's Homepage">
  <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
  <!-- Scramble Script by Jeff Donahue -->
  <script src="js/scramble.js"></script>
</head>

<body>
<table width="840" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr><td>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <p align="center">
    <!-- <font size="7">Rawal Khirodkar</font><br> -->
    <pageheading>Rawal Khirodkar</pageheading><br>
    <b>email</b>:&nbsp
    <font id="email" style="display:inline;">
      <noscript><i>Please enable Javascript to view</i></noscript>
    </font>
    <script>
    emailScramble = new scrambledString(document.getElementById('email'),
        'emailScramble', 'dumekc@ohr.iud.skrc',
        [7, 15, 14, 17, 2, 10, 9, 6, 3, 5, 16, 4, 19, 18, 12, 11, 8, 1, 13]);
    </script>
  </p>
<!-- rkhirodk -->
  <tr>
    <!-- <td width="32%" valign="top"><a href="images/rawal.jpg"><img src="images/rawal.jpg" width="100%" style="border-radius:15px"></a> -->
    <td width="32%" valign="top"><a href="images/rawal.jpeg"><img src="images/rawal.jpeg" width="100%" style="border-radius:15px"></a>
    <p align=center>
<!--     | <a href="docs/resume.pdf">Resume</a> |-->   
     | <a href="docs/cv.pdf">CV</a> |    
    <a href="https://scholar.google.com/citations?user=hS6M138AAAAJ&hl=en&oi=ao">Google Scholar</a> |<br/>|
    <a href="https://github.com/rawalkhirodkar">Github</a> |
    <a href="https://twitter.com/me_rawal">Twitter</a> |
    </p>
    </td>
    <td width="68%" valign="top" align="justify">
    <p>I am a Ph.D. student (2019-) at <a href="https://www.ri.cmu.edu/" target="_blank">Robotics Institute, Carnegie Mellon University</a>, advised by 
      Prof. <a href="http://www.cs.cmu.edu/~kkitani/" target="_blank">Kris Kitani</a>. Earlier, I received my <a href="docs/MSR_Thesis_Rawal.pdf"> Masters</a> (2017-2019) at CMU working with 
      Prof. Kris Kitani and obtained my <a href="docs/Btech_Report_Rawal.pdf"> B.Tech</a> (2013-2017) in computer science from <a href="https://www.cse.iitb.ac.in/" target="_blank"> IIT Bombay</a> working with Prof. <a href="https://www.cse.iitb.ac.in/~ganesh/" target="_blank">Ganesh Ramakrishnan</a>. I am a recipient of 2020 <a href="https://www.amazon.com/b?node=16008589011" target="_blank">Amazon Fellowship</a>. My graduate study is supported by <a href="https://www.mhrd.gov.in/" target="_blank">Gov. of India</a> and <a href="https://www.iarpa.gov/" target="_blank">IARPA</a>.
    </p>

    <p>My research is focused on the development of computer vision algorithms to understand human interactions with other humans and the real world. My current projects span developing methods and infrastructure to obtain data useful in estimating 2D/3D pose of humans under complex interaction scenarios. These projects are part of my longer term goal of improving the ways in which we model human-to-human interactions in the real world. These interaction models can then be used to simulate multi-human interactions and behavior in social settings.
    </td>
  </tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td>
    <sectionheading>&nbsp;&nbsp;News</sectionheading>
    <ul>
    <li> June, 2023. <a href="https://rawalkhirodkar.github.io/egohumans">Check out our new work - EgoHumans!</a>.</li>
    <li> Feb, 2023. <a href="https://cvpr2023.thecvf.com/">OCSORT accepted to CVPR 2023.</a>.</li>
    <li> Nov, 2022. <a href="https://youtu.be/VI4o0d_oWP4?list=PLw28TsbHRy4RrYFqETfGv7FHQ7L6EYO8-&t=13">Completed my Ph.D. proposal</a>.</li>
    <li> Aug, 2022. <a href="https://about.facebook.com/realitylabs/">Completed internship at Meta Reality Labs</a>.</li>
    <li> March, 2022. <a href="https://cvpr2022.thecvf.com/">OCHMR accepted to CVPR 2022</a>.</li>
    <li> July, 2021. <a href="http://iccv2021.thecvf.com/home/">Two papers accepted to ICCV 2021</a>.</li>
    </ul>
  </td></tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Publications</sectionheading></td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">

<tr>
    <td width="40%" valign="top"><a href="egohumans/files/thumbnail.png"><img src="egohumans/files/thumbnail.png" alt="sym" width="90%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p><a href="egohumans/files/paper.pdf" id="preprint_2023">
      <heading>EgoHumans: An Egocentric 3D Multi-Human Benchmark</heading></a><br>
      <strong>Rawal Khirodkar</strong>, Aayush Bansal, Lingni Ma, Richard Newcombe, Minh Vo, Kris Kitani<br>
      preprint 2023
      </p>

      <div class="paper" id="egohumans">
        <a href="https://rawalkhirodkar.github.io/egohumans">project page</a> |
        <a href="egohumans/files/paper.pdf" target="_blank">pdf</a> |
        <a href="javascript:toggleblock('egohumans_abs')">abstract</a> |
        <a shape="rect" href="javascript:togglebib('egohumans')" class="togglebib">bibtex</a> |
        <p align="justify"> <i id="egohumans_abs">We present EgoHumans, a new multi-view multi-human video benchmark to advance the state-of-the-art of egocentric human 3D pose estimation and tracking. Existing egocentric benchmarks either capture single subject or indoor-only scenarios, which limit the generalization of computer vision algorithms for real-world applications. We propose a novel 3D capture setup to construct a comprehensive egocentric multi-human benchmark in the wild with annotations to support diverse tasks such as human detection, tracking, 2D/3D pose estimation, and mesh recovery. We leverage consumer-grade wearable camera-equipped glasses for the egocentric view, which enables us to capture dynamic activities like playing soccer, fencing, volleyball, etc. Furthermore, our multi-view setup generates accurate 3D ground truth even under severe or complete occlusion. The dataset consists of more than 125k egocentric images, spanning diverse scenes with a particular focus on challenging and unchoreographed multi-human activities and fast-moving egocentric views. We rigorously evaluate existing state-of-the-art methods and highlight their limitations in the egocentric scenario, specifically on multi-human tracking. To address such limitations, we propose EgoFormer, a novel approach with a multi-stream transformer architecture and explicit 3D spatial reasoning to estimate and track the human pose. EgoFormer significantly outperforms prior art by 13.6 IDF1 and 9.3 HOTA on the EgoHumans dataset.</i></p>
        <pre xml:space="preserve">
         WIP
        </pre>
      </div>
    </td>
  </tr>

<tr>
    <td width="40%" valign="top"><a href="images/ocsort.png"><img src="images/ocsort.png" alt="sym" width="90%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p><a href="https://arxiv.org/abs/2203.14360" id="cvpr_2023">
      <heading>Observation-Centric SORT: Rethinking SORT for Robust Multi-Object Tracking</heading></a><br>
      Jinkun Cao, Xinshuo Weng, <strong>Rawal Khirodkar</strong>, Jiangmiao Pang, Kris Kitani<br>
      CVPR 2023
      </p>

      <div class="paper" id="ocsort">
        <a href="https://arxiv.org/abs/2203.14360" target="_blank">pdf</a> |
        <a href="javascript:toggleblock('ocsort_abs')">abstract</a> |
        <a shape="rect" href="javascript:togglebib('ocsort')" class="togglebib">bibtex</a> |
        <a href="https://github.com/noahcao/OC_SORT">code</a> 

        <p align="justify"> <i id="ocsort_abs">Multi-Object Tracking (MOT) has rapidly progressed with the development of object detection and re-identification. However, motion modeling, which facilitates object association by forecasting short-term trajectories with past observations, has been relatively under-explored in recent years. Current motion models in MOT typically assume that the object motion is linear in a small time window and needs continuous observations, so these methods are sensitive to occlusions and non-linear motion and require high frame-rate videos. In this work, we show that a simple motion model can obtain state-of-the-art tracking performance without other cues like appearance. We emphasize the role of "observation" when recovering tracks from being lost and reducing the error accumulated by linear motion models during the lost period. We thus name the proposed method as Observation-Centric SORT, OC-SORT for short. It remains simple, online, and real-time but improves robustness over occlusion and non-linear motion. It achieves 63.2 and 62.1 HOTA on MOT17 and MOT20, respectively, surpassing all published methods. It also sets new states of the art on KITTI Pedestrian Tracking and DanceTrack where the object motion is highly non-linear.</i></p>

        <pre xml:space="preserve">
         @article{cao2022observation,
            title={Observation-centric sort: Rethinking sort for robust multi-object tracking},
            author={Cao, Jinkun and Weng, Xinshuo and Khirodkar, Rawal and Pang, Jiangmiao and Kitani, Kris},
            journal={arXiv preprint arXiv:2203.14360},
            year={2022}
          }
        </pre>
      </div>
    </td>
  </tr>

<tr>
    <td width="40%" valign="top"><a href="images/seq_ens.png"><img src="images/seq_ens.png" alt="sym" width="90%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p><a href="https://arxiv.org/abs/2210.05387" id="arxiv_2022">
      <heading>Sequential Ensembling for Semantic Segmentation</heading></a><br>
      <strong>Rawal Khirodkar</strong>, Brandon Smith, Siddhartha Chandra, Amit Agrawal, Antonio Criminisi<br>
      preprint
      </p>

      <div class="paper" id="seq_ens">
        <a href="https://arxiv.org/pdf/2210.05387.pdf" target="_blank">pdf</a> |
        <a href="javascript:toggleblock('seq_ens_abs')">abstract</a> |
        <a shape="rect" href="javascript:togglebib('seq_ens')" class="togglebib">bibtex</a> 

        <p align="justify"> <i id="seq_ens_abs">Ensemble approaches for deep-learning-based semantic segmentation remain insufficiently explored despite the proliferation of competitive benchmarks and downstream applications. In this work, we explore and benchmark the popular ensembling approach of combining predictions of multiple, independently-trained, state-of-the-art models at test time on popular datasets. Furthermore, we propose a novel method inspired by boosting to sequentially ensemble networks that significantly outperforms the naive ensemble baseline. Our approach trains a cascade of models conditioned on class probabilities predicted by the previous model as an additional input. A key benefit of this approach is that it allows for dynamic computation offloading, which helps deploy models on mobile devices. Our proposed novel ADaptive modulatiON (ADON) block allows spatial feature modulation at various layers using previous-stage probabilities. Our approach does not require sophisticated sample selection strategies during training and works with multiple neural architectures. We significantly improve over the naive ensemble baseline on challenging datasets such as Cityscapes, ADE-20K, COCO-Stuff, and PASCAL-Context and set a new state-of-the-art.</i></p>

        <pre xml:space="preserve">
         @article{khirodkar2022sequential,
              title={Sequential Ensembling for Semantic Segmentation},
              author={Khirodkar, Rawal and Smith, Brandon and Chandra, Siddhartha and Agrawal, Amit and Criminisi, Antonio},
              journal={arXiv preprint arXiv:2210.05387},
              year={2022}
            }
        </pre>
      </div>
    </td>
  </tr>

  <tr>
    <td width="40%" valign="top"><a href="images/cvpr22_ochmr.png"><img src="images/cvpr22_ochmr.png" alt="sym" width="90%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p><a href="https://arxiv.org/abs/2203.13349" id="CVPR22">
      <heading>Occluded Human Mesh Recovery</heading></a><br>
      <strong>Rawal Khirodkar</strong>, Shashank Tripathi, Kris Kitani<br>
      CVPR 2022
      </p>

      <div class="paper" id="cvpr22_ochmr">
        <a href="https://rawalkhirodkar.github.io/ochmr/">project page</a> |
        <a href="ochmr/files/paper.pdf" target="_blank">pdf</a> |
        <a href="javascript:toggleblock('cvpr22_ochmr_abs')">abstract</a> |
        <a shape="rect" href="javascript:togglebib('cvpr22_ochmr')" class="togglebib">bibtex</a> 

        <p align="justify"> <i id="cvpr22_ochmr_abs">Top-down methods for monocular human mesh recovery have two stages: (1) detect human bounding boxes; (2) treat each bounding box as an independent single-human mesh recovery task. Unfortunately, the single-human assumption does not hold in images with multi-human occlusion and crowding. Consequently, top-down methods have difficulties in recovering accurate 3D human meshes under severe person-person occlusion. To address this, we present Occluded Human Mesh Recovery (OCHMR) - a novel top-down mesh recovery approach that incorporates image spatial context to overcome the limitations of the single-human assumption. The approach is conceptually simple and can be applied to any existing top-down architecture. Along with the input image, we condition the top-down model on spatial context from the image in the form of body-center heatmaps. To reason from the predicted body centermaps, we introduce Contextual Normalization (CoNorm) blocks to adaptively modulate intermediate features of the top-down model. The contextual conditioning helps our model disambiguate between two severely overlapping human bounding-boxes, making it robust to multi-person occlusion. Compared with state-of-the-art methods, OCHMR achieves superior performance on challenging multi-person benchmarks like 3DPW, CrowdPose and OCHuman. Specifically, our proposed contextual reasoning architecture applied to the SPIN model with ResNet-50 backbone results in 75.2 PMPJPE on 3DPW-PC, 23.6 AP on CrowdPose and 37.7 AP on OCHuman datasets, a significant improvement of 6.9 mm, 6.4 AP and 20.8 AP respectively over the baseline.</i></p>

        <pre xml:space="preserve">
         @article{khirodkar2022occluded,
          title={Occluded Human Mesh Recovery},
          author={Khirodkar, Rawal and Tripathi, Shashank and Kitani, Kris},
          journal={arXiv preprint arXiv:2203.13349},
          year={2022}
        }
        </pre>
      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top"><a href="images/iccv21_mip.png"><img src="images/iccv21_mip.png" alt="sym" width="90%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p><a href="https://arxiv.org/abs/2101.11223" id="ICCV21">
      <heading>Multi-Instance Pose Networks: Rethinking Top-Down Pose Estimation</heading></a><br>
      <strong>Rawal Khirodkar</strong>, Visesh Chari, Amit Agrawal, Ambrish Tyagi<br>
      ICCV 2021
      </p>

      <div class="paper" id="iccv21_mip">
        <a href="https://rawalkhirodkar.github.io/mipnet/">project page</a> |
        <a href="https://www.amazon.science/publications/multi-instance-pose-networks-rethinking-top-down-pose-estimation">Amazon Science</a> |
        <a href="mipnet/files/paper.pdf" target="_blank">pdf</a> |
        <a href="javascript:toggleblock('iccv21_mip_abs')">abstract</a> |
        <a shape="rect" href="javascript:togglebib('iccv21_mip')" class="togglebib">bibtex</a> |
        <a href="https://github.com/rawalkhirodkar/MIPNet">code</a> 

        <p align="justify"> <i id="iccv21_mip_abs">A key assumption of top-down human pose estimation approaches is their expectation of having a single person/instance present in the input bounding box. This often leads to failures in crowded scenes with occlusions. We propose a novel solution to overcome the limitations of this fundamental assumption. Our Multi-Instance Pose Network (MIPNet) allows for predicting multiple 2D pose instances within a given bounding box. We introduce a Multi-Instance Modulation Block (MIMB) that can adaptively modulate channel-wise feature responses for each instance and is parameter efficient. We demonstrate the efficacy of our approach by evaluating on COCO, CrowdPose, and OCHuman datasets. Specifically, we achieve 70.0 AP on CrowdPose and 42.5 AP on OCHuman test sets, a significant improvement of 2.4 AP and 6.5 AP over the prior art, respectively. When using ground truth bounding boxes for inference, MIPNet achieves an improvement of 0.7 AP on COCO, 0.9 AP on CrowdPose, and 9.1 AP on OCHuman validation sets compared to HRNet. Interestingly, when fewer, high confidence bounding boxes are used, HRNet's performance degrades (by 5 AP) on OCHuman, whereas MIPNet maintains a relatively stable performance (drop of 1 AP) for the same inputs.</i></p>

        <pre xml:space="preserve">
         @InProceedings{Khirodkar_2021_ICCV,
              author    = {Khirodkar, Rawal and Chari, Visesh and Agrawal, Amit and Tyagi, Ambrish},
              title     = {Multi-Instance Pose Networks: Rethinking Top-Down Pose Estimation},
              booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
              month     = {October},
              year      = {2021},
              pages     = {3122-3131}
          }
        </pre>
      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top"><a href="images/iccv21_repose.png"><img src="images/iccv21_repose.png" alt="sym" width="90%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p><a href="https://arxiv.org/pdf/2104.00633" id="ICCV21">
      <heading>RePOSE: Fast 6D Object Pose Refinement via Deep Texture Rendering</heading></a><br>
      Shun Iwase, Xingyu Liu, <strong>Rawal Khirodkar</strong>, Rio Yokota, Kris M. Kitani<br>
      ICCV 2021
      </p>

      <div class="paper" id="iccv21_repose">
        <a href="https://arxiv.org/pdf/2104.00633.pdf" target="_blank">pdf</a> |
        <a href="javascript:toggleblock('iccv21_repose_abs')">abstract</a> |
        <a shape="rect" href="javascript:togglebib('iccv21_repose')" class="togglebib">bibtex</a> |
        <a href="https://github.com/sh8/RePOSE">code</a> 

        <p align="justify"> <i id="iccv21_repose_abs">The use of iterative pose refinement is a critical processing step for 6D object pose estimation, and its performance depends greatly on one's choice of image representation. Image representations learned via deep convolutional neural networks (CNN) are currently the method of choice as they are able to robustly encode object keypoint locations. However, CNN-based image representations are computational expensive to use for iterative pose refinement, as they require that image features are extracted using a deep network, once for the input image and multiple times for rendered images during the refinement process. Instead of using a CNN to extract image features from a rendered RGB image, we propose to directly render a deep feature image. We call this deep texture rendering, where a shallow multi-layer perceptron is used to directly regress a view invariant image representation of an object. Using an estimate of the pose and deep texture rendering, our system can render an image representation in under 1ms. This image representation is optimized such that it makes it easier to perform nonlinear 6D pose estimation by adding a differentiable Levenberg-Marquardt optimization network and back-propagating the 6D pose alignment error. We call our method, RePOSE, a Real-time Iterative Rendering and Refinement algorithm for 6D POSE estimation. RePOSE runs at 71 FPS and achieves state-of-the-art accuracy of 51.6% on the Occlusion LineMOD dataset - a 4.1% absolute improvement over the prior art, and comparable performance on the YCB-Video dataset with a much faster runtime than the other pose refinement methods.</i></p>

        <pre xml:space="preserve">
          @InProceedings{Iwase_2021_ICCV,
              author    = {Iwase, Shun and Liu, Xingyu and Khirodkar, Rawal and Yokota, Rio and Kitani, Kris M.},
              title     = {RePOSE: Fast 6D Object Pose Refinement via Deep Texture Rendering},
              booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
              month     = {October},
              year      = {2021},
              pages     = {3303-3312}
          }
        </pre>
      </div>
    </td>
  </tr>

  
  <tr>
    <td width="33%" valign="top"><a href="images/publications/adr/1.png"><img src="images/publications/adr/1.png" alt="sym" width="90%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p><a href="https://arxiv.org/pdf/1812.00491" id="ADR">
      <heading>Adversarial Domain Randomization</heading></a><br>
      <strong>Rawal Khirodkar</strong>, Kris M. Kitani<br>
      preprint
      </p>

      <div class="paper" id="adr">
        <a href="https://arxiv.org/pdf/1812.00491.pdf" target="_blank">pdf</a> |
        <a href="javascript:toggleblock('adr_abs')">abstract</a> |
        <a shape="rect" href="javascript:togglebib('adr')" class="togglebib">bibtex</a>

        <p align="justify"> <i id="adr_abs">Domain Randomization (DR) is known to require a significant amount of training data for good performance. We argue that this is due to DR's strategy of random data generation using a uniform distribution over simulation parameters, as a result, DR often generates samples which are uninformative for the learner. In this work, we theoretically analyze DR using ideas from multi-source domain adaptation. Based on our findings, we propose Adversarial Domain Randomization (ADR) as an efficient variant of DR which generates adversarial samples with respect to the learner during training. We implement ADR as a policy whose action space is the quantized simulation parameter space. At each iteration, the policy's action generates labeled data and the reward is set as negative of learner's loss on this data. As a result, we observe ADR frequently generates novel samples for the learner like truncated and occluded objects for object detection and confusing classes for image classification. We perform evaluations on datasets like CLEVR, Syn2Real, and VIRAT for various tasks where we demonstrate that ADR outperforms DR by generating fewer data samples.</i></p>

        <pre xml:space="preserve">
          @article{DBLP:journals/corr/abs-1812-00491,
            author    = {Rawal Khirodkar and
                         Donghyun Yoo and
                         Kris M. Kitani},
            title     = {{VADRA:} Visual Adversarial Domain Randomization and Augmentation},
            journal   = {CoRR},
            volume    = {abs/1812.00491},
            year      = {2018},
            url       = {http://arxiv.org/abs/1812.00491},
            archivePrefix = {arXiv},
            eprint    = {1812.00491},
            timestamp = {Tue, 01 Jan 2019 15:01:25 +0100},
            biburl    = {https://dblp.org/rec/journals/corr/abs-1812-00491.bib},
            bibsource = {dblp computer science bibliography, https://dblp.org}
          }
        </pre>
      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top"><a href="images/wacv19.png"><img src="images/wacv19.png" alt="sym" width="90%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p><a href="https://arxiv.org/abs/1811.05939" id="WACV19">
      <heading>Domain Randomization for Scene-Specific Car Detection and Pose Estimation</heading></a><br>
      <strong>Rawal Khirodkar</strong>, Donghyun Yoo, Kris M. Kitani<br>
      WACV 2019
      </p>

      <div class="paper" id="wacv19_dr">
        <a href="https://arxiv.org/pdf/1811.05939.pdf" target="_blank">pdf</a> |
        <a href="javascript:toggleblock('wacv19_dr_abs')">abstract</a> |
        <a shape="rect" href="javascript:togglebib('wacv19_dr')" class="togglebib">bibtex</a> |
        <a href="https://github.com/rawalkhirodkar/car_pose">code</a> 

        <p align="justify"> <i id="wacv19_dr_abs">We address the issue of domain gap when making use of synthetic data to train a scene-specific object detector and pose estimator. While previous works have shown that the constraints of learning a scene-specific model can be leveraged to create geometrically and photometrically consistent synthetic data, care must be taken to design synthetic content which is as close as possible to the real-world data distribution. In this work, we propose to solve domain gap through the use of appearance randomization to generate a wide range of synthetic objects to span the space of realistic images for training. An ablation study of our results is presented to delineate the individual contribution of different components in the randomization process. We evaluate our method on VIRAT, UA-DETRAC, EPFL-Car datasets, where we demonstrate that using scene specific domain randomized synthetic data is better than fine-tuning off-the-shelf models on limited real data.</i></p>

        <pre xml:space="preserve">
          @inproceedings{khirodkar2019domain,
            title={Domain randomization for scene-specific car detection and pose estimation},
            author={Khirodkar, Rawal and Yoo, Donghyun and Kitani, Kris},
            booktitle={2019 IEEE Winter Conference on Applications of Computer Vision (WACV)},
            pages={1932--1940},
            year={2019},
            organization={IEEE}
          }
        </pre>
      </div>
    </td>
  </tr>

</table>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Teaching</sectionheading></td></tr>
</table>
<table width="100%" align="top" border="0" cellpadding="20">
  <tr>
    <td valign="top">
      <p>
        <a href="http://www.cs.cmu.edu/~me/811/"><heading>16-720: Computer Vision - Fall 2022, Fall 2021, Fall 2020, Spring 2018</heading></a><br>
        <strong>Instructor</strong>: Prof. Kris Kitani<br>
      </p>

      <p>
        <a href="http://www.cs.cmu.edu/~me/811/"><heading>16-811: Math Fundamental for Robotics - Fall, 2019</heading></a><br>
        <strong>Instructor</strong>: Prof. Michael Erdmann<br>
      </p>

      <p>
        <a href="http://www.cs.cmu.edu/~kkitani/class/2018/05/15/F18-16831/"><heading>16-831: Statistical Techniques in Robotics - Spring, 2019</heading></a><br>
        <strong>Instructor</strong>: Prof. Kris Kitani<br>
      </p>

      <p>
        <a href="http://www.cs.cmu.edu/~mgormley/courses/10601/"><heading>10-601: Introduction to Machine Learning - Fall, 2018</heading></a><br>
        <strong>Instructor</strong>: Prof. Matt Gormley<br>
      </p>

    </td>
  </tr>
</table>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Miscellaneous</sectionheading></td></tr>
  <tr><td>&nbsp;&nbsp;Learning to play with blender in my spare time. Checkout some of my projects <a href="https://github.com/rawalkhirodkar/blender">here</a>.</td></tr>

</table>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td> <img src="images/misc/ybot.gif" width="1024" height="576"> </td></tr>
  <tr><td> <img src="images/misc/boxing.gif" width="1024" height="576"> </td></tr>
</table>



<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
    <tr><td><br><p align="right"><font size="1.5">
    Template modified from <a href="http://www.cs.berkeley.edu/~barron/">this</a>, and <a href="http://jeffdonahue.com/">this</a>
    </font></p></td></tr>
</table>

  </td></tr>
</table>



<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>

<script xml:space="preserve" language="JavaScript">
hideblock('egohumans_abs');
hideblock('ocsort_abs');
hideblock('seq_ens_abs');
hideblock('cvpr22_ochmr_abs');
hideblock('iccv21_mip_abs');
hideblock('iccv21_repose_abs');
hideblock('adr_abs');
hideblock('wacv19_dr_abs');
</script>

</body>

</html>
