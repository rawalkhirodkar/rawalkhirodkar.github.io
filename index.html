
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <!-- Start : Google Analytics Code -->
  <!-- <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-177079629-1', 'auto');
    ga('send', 'pageview');
  </script> -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-177079629-1"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-177079629-1');
  </script>

  <!-- End : Google Analytics Code -->
  
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* Design Credits: Jon Barron and Abhishek Kar and Saurabh Gupta*/
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 400
  }
  heading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 17px; /* 19 */
    font-weight: 600 /* 1000 */
  }
  strong {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 600 /* 800 */
  }
  strongred {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    color: 'red' ;
    font-size: 16px
  }
  sectionheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    font-weight: 600
  }
  pageheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 38px;
    font-weight: 400
  }
  .ImageBorder
  {
      border-width: 1px;
      border-color: Black;
  }
  </style>
  <link rel="shortcut icon" href="https://www.cs.cmu.edu/sites/default/files/favicon_0.ico" type="image/vnd.microsoft.icon">
  <script type="text/javascript" src="js/hidebib.js"></script>
  <title>Rawal Khirodkar</title>
  <meta name="Rawal Khirodkar's Homepage" http-equiv="Content-Type" content="Rawal Khirodkar's Homepage">
  <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
  <!-- Scramble Script by Jeff Donahue -->
  <script src="js/scramble.js"></script>
</head>

<body>
<table width="840" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr><td>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <p align="center">
    <!-- <font size="7">Rawal Khirodkar</font><br> -->
    <pageheading>Rawal Khirodkar</pageheading><br>
    <b>email</b>:&nbsp
    <font id="email" style="display:inline;">
      <noscript><i>Please enable Javascript to view</i></noscript>
    </font>
    <script>
    emailScramble = new scrambledString(document.getElementById('email'),
        'emailScramble', 'dumekc@ohr.iud.skrc',
        [7, 15, 14, 17, 2, 10, 9, 6, 3, 5, 16, 4, 19, 18, 12, 11, 8, 1, 13]);
    </script>
  </p>
<!-- rkhirodk -->
  <tr>
    <td width="32%" valign="top"><a href="images/rawal.jpg"><img src="images/rawal.jpg" width="100%" style="border-radius:15px"></a>
    <p align=center>
    | <a href="docs/resume.pdf">Resume</a> |
    <a href="https://scholar.google.com/citations?user=hS6M138AAAAJ&hl=en&oi=ao">Google Scholar</a> |<br/>|
    <a href="https://github.com/rawalkhirodkar">Github</a> |
    <a href="https://twitter.com/me_rawal">Twitter</a> |
    <a href="https://www.instagram.com/rawalkhirodkar/">Instagram</a> |
    </p>
    </td>
    <td width="68%" valign="top" align="justify">
    <p>I am a Ph.D. student (2019-) at <a href="https://www.ri.cmu.edu/" target="_blank">Robotics Institute, Carnegie Mellon University</a>, advised by 
      Prof. <a href="http://www.cs.cmu.edu/~kkitani/" target="_blank">Kris Kitani</a>. Earlier, I received my <a href="docs/MSR_Thesis_Rawal.pdf"> Masters</a> (2017-2019) at CMU working 
      Prof. Kris Kitani and obtained my <a href="docs/Btech_Report_Rawal.pdf"> B.Tech</a> (2013-2017) in computer science from <a href="https://www.cse.iitb.ac.in/" target="_blank"> IIT Bombay</a> working with Prof. <a href="https://www.cse.iitb.ac.in/~ganesh/" target="_blank">Ganesh Ramakrishnan</a>. I am a recipient of 2019 <a href="https://www.amazon.com/b?node=16008589011" target="_blank">Amazon Go Fellowship</a>. My graduate study is supported partly by <a href="https://www.mhrd.gov.in/" target="_blank">Gov. of India</a> and <a href="https://www.iarpa.gov/" target="_blank">IARPA</a>.
    </p>

    <p>I work in Artificial Intelligence at the intersection of Computer Vision, Machine Learning & Robotics. My research is focused on efficient use of synthetic data for problems in computer vision
      where annotations are scarcely available. 
    </td>
  </tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td>
    <sectionheading>&nbsp;&nbsp;News</sectionheading>
    <ul>
    <li> July, 2021. <a href="http://iccv2021.thecvf.com/home/">Two papers accepted to ICCV 2021</a>.</li>
    <li> July, 2021. <a href="https://amazon.jobs/en/teams/lab126/">Research Internship at Amazon Lab126, CA</a>.</li>
    <!-- <li> Aug, 2020. <a href="https://amazon.jobs/en/teams/lab126/">Research Internship at Amazon Lab126, CA</a>.</li> -->
    <!-- <li> Nov, 2019. <a href="https://www.amazon.com/b?node=16008589011">Awarded Amazon Go Fellowship</a>.</li> -->
    <!-- <li> Apr, 2019. <a href="docs/MSR_Thesis_Rawal.pdf">Defended by master thesis at CMU</a>.</li> -->
    <!-- <li> Jan, 2019. <a href="https://ieeexplore.ieee.org/document/8658387">Paper accepted to WACV 2019</a>.</li> -->
    </ul>
  </td></tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Publications</sectionheading></td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">

  <tr>
    <td width="33%" valign="top"><a href="images/iccv21_mip.png"><img src="images/iccv21_mip.png" alt="sym" width="90%" height="200%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p><a href="https://arxiv.org/abs/2101.11223" id="ICCV21">
      <heading>Multi-Instance Pose Networks: Rethinking Top-Down Pose Estimation</heading></a><br>
      <strong>Rawal Khirodkar</strong>, Visesh Chari, Amit Agrawal, Ambrish Tyagi<br>
      ICCV 2021
      </p>

      <div class="paper" id="iccv21_mip">
        <a href="https://arxiv.org/abs/2101.11223.pdf">pdf</a> |
        <a href="javascript:toggleblock('iccv21_mip_abs')">abstract</a> |
        <a shape="rect" href="javascript:togglebib('iccv21_mip')" class="togglebib">bibtex</a> |

        <p align="justify"> <i id="iccv21_mip_abs">A key assumption of top-down human pose estimation approaches is their expectation of having a single person/instance present in the input bounding box. This often leads to failures in crowded scenes with occlusions. We propose a novel solution to overcome the limitations of this fundamental assumption. Our Multi-Instance Pose Network (MIPNet) allows for predicting multiple 2D pose instances within a given bounding box. We introduce a Multi-Instance Modulation Block (MIMB) that can adaptively modulate channel-wise feature responses for each instance and is parameter efficient. We demonstrate the efficacy of our approach by evaluating on COCO, CrowdPose, and OCHuman datasets. Specifically, we achieve 70.0 AP on CrowdPose and 42.5 AP on OCHuman test sets, a significant improvement of 2.4 AP and 6.5 AP over the prior art, respectively. When using ground truth bounding boxes for inference, MIPNet achieves an improvement of 0.7 AP on COCO, 0.9 AP on CrowdPose, and 9.1 AP on OCHuman validation sets compared to HRNet. Interestingly, when fewer, high confidence bounding boxes are used, HRNet's performance degrades (by 5 AP) on OCHuman, whereas MIPNet maintains a relatively stable performance (drop of 1 AP) for the same inputs.</i></p>

        <pre xml:space="preserve">
          @article{DBLP:journals/corr/abs-2101-11223,
            author    = {Rawal Khirodkar and
                         Visesh Chari and
                         Amit K. Agrawal and
                         Ambrish Tyagi},
            title     = {Multi-Hypothesis Pose Networks: Rethinking Top-Down Pose Estimation},
            journal   = {CoRR},
            volume    = {abs/2101.11223},
            year      = {2021},
            url       = {https://arxiv.org/abs/2101.11223},
            archivePrefix = {arXiv},
            eprint    = {2101.11223},
            timestamp = {Sun, 31 Jan 2021 17:23:50 +0100},
            biburl    = {https://dblp.org/rec/journals/corr/abs-2101-11223.bib},
            bibsource = {dblp computer science bibliography, https://dblp.org}
          }
        </pre>
      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top"><a href="images/iccv21_repose.png"><img src="images/iccv21_repose.png" alt="sym" width="90%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p><a href="https://arxiv.org/abs/1811.05939" id="ICCV21">
      <heading>RePOSE: Real-Time Iterative Rendering and Refinement for 6D Object Pose Estimation</heading></a><br>
      Shun Iwase, Xingyu Liu, <strong>Rawal Khirodkar</strong>, Rio Yokota, Kris M. Kitani<br>
      ICCV 2021
      </p>

      <div class="paper" id="iccv21_repose">
        <a href="https://arxiv.org/pdf/2104.00633.pdf">pdf</a> |
        <a href="javascript:toggleblock('iccv21_repose_abs')">abstract</a> |
        <a shape="rect" href="javascript:togglebib('iccv21_repose')" class="togglebib">bibtex</a> |

        <p align="justify"> <i id="iccv21_repose_abs">The use of iterative pose refinement is a critical processing step for 6D object pose estimation, and its performance depends greatly on one's choice of image representation. Image representations learned via deep convolutional neural networks (CNN) are currently the method of choice as they are able to robustly encode object keypoint locations. However, CNN-based image representations are computational expensive to use for iterative pose refinement, as they require that image features are extracted using a deep network, once for the input image and multiple times for rendered images during the refinement process. Instead of using a CNN to extract image features from a rendered RGB image, we propose to directly render a deep feature image. We call this deep texture rendering, where a shallow multi-layer perceptron is used to directly regress a view invariant image representation of an object. Using an estimate of the pose and deep texture rendering, our system can render an image representation in under 1ms. This image representation is optimized such that it makes it easier to perform nonlinear 6D pose estimation by adding a differentiable Levenberg-Marquardt optimization network and back-propagating the 6D pose alignment error. We call our method, RePOSE, a Real-time Iterative Rendering and Refinement algorithm for 6D POSE estimation. RePOSE runs at 71 FPS and achieves state-of-the-art accuracy of 51.6% on the Occlusion LineMOD dataset - a 4.1% absolute improvement over the prior art, and comparable performance on the YCB-Video dataset with a much faster runtime than the other pose refinement methods.</i></p>

        <pre xml:space="preserve">
          @article{DBLP:journals/corr/abs-2104-00633,
            author    = {Shun Iwase and
                         Xingyu Liu and
                         Rawal Khirodkar and
                         Rio Yokota and
                         Kris M. Kitani},
            title     = {RePOSE: Real-Time Iterative Rendering and Refinement for 6D Object
                         Pose Estimation},
            journal   = {CoRR},
            volume    = {abs/2104.00633},
            year      = {2021},
            url       = {https://arxiv.org/abs/2104.00633},
            archivePrefix = {arXiv},
            eprint    = {2104.00633},
            timestamp = {Mon, 12 Apr 2021 16:14:56 +0200},
            biburl    = {https://dblp.org/rec/journals/corr/abs-2104-00633.bib},
            bibsource = {dblp computer science bibliography, https://dblp.org}
          }
        </pre>
      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top"><a href="images/wacv19.png"><img src="images/wacv19.png" alt="sym" width="90%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p><a href="https://arxiv.org/abs/1811.05939" id="WACV19">
      <heading>Domain Randomization for Scene-Specific Car Detection and Pose Estimation</heading></a><br>
      <strong>Rawal Khirodkar</strong>, Donghyun Yoo, Kris M. Kitani<br>
      WACV 2019
      </p>

      <div class="paper" id="wacv19_dr">
        <a href="https://arxiv.org/pdf/1811.05939.pdf">pdf</a> |
        <a href="javascript:toggleblock('wacv19_dr_abs')">abstract</a> |
        <a shape="rect" href="javascript:togglebib('wacv19_dr')" class="togglebib">bibtex</a> |
        <a href="https://github.com/rawalkhirodkar/car_pose">code</a> |

        <p align="justify"> <i id="wacv19_dr_abs">We address the issue of domain gap when making use of synthetic data to train a scene-specific object detector and pose estimator. While previous works have shown that the constraints of learning a scene-specific model can be leveraged to create geometrically and photometrically consistent synthetic data, care must be taken to design synthetic content which is as close as possible to the real-world data distribution. In this work, we propose to solve domain gap through the use of appearance randomization to generate a wide range of synthetic objects to span the space of realistic images for training. An ablation study of our results is presented to delineate the individual contribution of different components in the randomization process. We evaluate our method on VIRAT, UA-DETRAC, EPFL-Car datasets, where we demonstrate that using scene specific domain randomized synthetic data is better than fine-tuning off-the-shelf models on limited real data.</i></p>

        <pre xml:space="preserve">
          @inproceedings{khirodkar2019domain,
            title={Domain randomization for scene-specific car detection and pose estimation},
            author={Khirodkar, Rawal and Yoo, Donghyun and Kitani, Kris},
            booktitle={2019 IEEE Winter Conference on Applications of Computer Vision (WACV)},
            pages={1932--1940},
            year={2019},
            organization={IEEE}
          }
        </pre>
      </div>
    </td>
  </tr>

</table>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Teaching</sectionheading></td></tr>
</table>
<table width="100%" align="top" border="0" cellpadding="20">
  <tr>
    <td valign="top">
      <p>
        <a href="http://www.cs.cmu.edu/~me/811/"><heading>16-720: Computer Vision - Spring, 2021</heading></a><br>
        <strong>Instructor</strong>: Prof. Kris Kitani<br>
      </p>

      <p>
        <a href="http://www.cs.cmu.edu/~me/811/"><heading>16-811: Math Fundamental for Robotics - Fall, 2019</heading></a><br>
        <strong>Instructor</strong>: Prof. Michael Erdmann<br>
      </p>

      <p>
        <a href="http://www.cs.cmu.edu/~kkitani/class/2018/05/15/F18-16831/"><heading>16-831: Statistical Techniques in Robotics - Spring, 2019</heading></a><br>
        <strong>Instructor</strong>: Prof. Kris Kitani<br>
      </p>

      <p>
        <a href="http://www.cs.cmu.edu/~mgormley/courses/10601/"><heading>10-601: Introduction to Machine Learning - Fall, 2018</heading></a><br>
        <strong>Instructor</strong>: Prof. Matt Gormley<br>
      </p>
      <p>
        <a href="http://16720.courses.cs.cmu.edu/"><heading>16-720: Computer Vision - Spring, 2018</heading></a><br>
        <strong>Instructor</strong>: Prof. Kris Kitani<br>
      </p>
    </td>
  </tr>
</table>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
    <tr><td><br><p align="right"><font size="1.5">
    Template modified from <a href="https://www.cs.cmu.edu/~dpathak/">this</a>, <a href="http://www.cs.berkeley.edu/~barron/">this</a>, and <a href="http://jeffdonahue.com/">this</a>
    </font></p></td></tr>
</table>

  </td></tr>
</table>


<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>

<script xml:space="preserve" language="JavaScript">
hideblock('iccv21_mip_abs');
hideblock('iccv21_repose_abs');
hideblock('wacv19_dr_abs');

</script>

</body>

</html>
